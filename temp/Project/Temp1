import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import io

# Import the dataset for review as a DataFrame
df = pd.read_csv("/content/dataset.csv")

# Review the first five observations
df.head()

# --- 1. Data Loading (Simulated from Image) ---

print("--- Data Loading Complete ---")
print("First 5 rows of the dataset:")
print(df.head())

# --- 2. Exploratory Data Analysis (EDA) ---
print("\n--- Exploratory Data Analysis (EDA) ---")

# Get basic information about the DataFrame
print("\nDataFrame Info:")
df.info()

# Check for missing values
print("\nMissing values (isnull().sum()):")
print(df.isnull().sum())

# Get the shape of the DataFrame (rows, columns)
print(f"\nShape of the dataset (rows, columns): {df.shape}")
rows, columns = df.shape
print(f"Number of rows: {rows}")
print(f"Number of columns: {columns}")

# Get column names
print("\nColumn names:")
print(df.columns.tolist())

# Descriptive statistics for numerical columns
print("\nDescriptive statistics (describe()):")
print(df.describe())

# Data type check
print("\nData types of columns:")
print(df.dtypes)

# Unique values for 'id' (assuming it's a site ID or similar)
print(f"\nUnique values in 'id' column: {df['id'].unique()}")

# Visualize distributions of numerical features
print("\nGenerating distribution plots for numerical features...")
numerical_cols = ['NH4', 'BSK5', 'Suspended', 'O2', 'NO3', 'NO2', 'SO4', 'PO4', 'CL']
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 3, i + 1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Correlation matrix
print("\nGenerating correlation heatmap...")
plt.figure(figsize=(10, 8))
sns.heatmap(df[numerical_cols].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Water Quality Parameters')
plt.show()

# --- 3. Data Preprocessing & Manipulation ---
print("\n--- Data Preprocessing & Manipulation ---")

# Convert 'date' column to datetime objects
df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y')
print("Converted 'date' column to datetime objects.")

# Extract time-based features (e.g., year, month, day of week) - Manipulation
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day_of_week'] = df['date'].dt.dayofweek
print("Extracted 'year', 'month', 'day_of_week' features from 'date'.")

# Drop original 'id' and 'date' columns as they are not direct features for prediction
# 'id' might be useful if you were doing time-series analysis per site, but for a general model,
# it's often dropped or used for grouping.
df = df.drop(columns=['id', 'date'])
print("Dropped 'id' and 'date' columns.")

# Define features (X) and target (y)
# We'll predict 'O2' (Oxygen) as a primary indicator of water quality.
# All other numerical columns (after dropping 'id' and 'date') will be features.
features = [col for col in df.columns if col != 'O2']
target = 'O2'

X = df[features]
y = df[target]

print(f"\nFeatures (X) columns: {X.columns.tolist()}")
print(f"Target (y) column: {target}")

# Handle missing values (if any, based on isnull().sum() from EDA)
# For simplicity, we'll fill numerical missing values with the mean.
# In a larger dataset, consider more sophisticated imputation strategies (e.g., KNNImputer).
print("\nHandling missing values (filling with mean)...")
for col in X.columns:
    if X[col].isnull().any():
        X[col] = X[col].fillna(X[col].mean())
        print(f"Filled missing values in '{col}' with its mean.")
if y.isnull().any():
    y = y.fillna(y.mean())
    print(f"Filled missing values in target '{target}' with its mean.")

print("\nMissing values after preprocessing:")
print(X.isnull().sum())
print(f"Target missing values: {y.isnull().sum()}")

# Scaling numerical features
# Scaling is crucial for distance-based algorithms like KNN.
print("\nScaling numerical features using StandardScaler...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns) # Convert back to DataFrame for consistency

print("Features scaled.")
print("Scaled X head:")
print(X_scaled_df.head())
# --- 4. Data Modeling ---
print("\n--- Data Modeling ---")

# Split the data into training and testing sets
# Using the scaled features for models that benefit from it (like KNN)
X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y, test_size=0.25, random_state=42)

print(f"\nTraining set size: {len(X_train)} samples")
print(f"Test set size: {len(X_test)} samples")

# --- Model 1: Random Forest Regressor ---
print("\n--- Training Random Forest Regressor ---")
rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1) # n_jobs=-1 uses all available cores
rf_model.fit(X_train, y_train)
print("Random Forest model trained.")

# Predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluation
mae_rf = mean_absolute_error(y_test, y_pred_rf)
mse_rf = mean_squared_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mse_rf)
r2_rf = r2_score(y_test, y_pred_rf)

print(f"Random Forest MAE: {mae_rf:.4f}")
print(f"Random Forest MSE: {mse_rf:.4f}")
print(f"Random Forest RMSE: {rmse_rf:.4f}")
print(f"Random Forest R2 Score: {r2_rf:.4f}")

# Cross-validation for Random Forest
print("\nPerforming 5-fold cross-validation for Random Forest...")
kf = KFold(n_splits=5, shuffle=True, random_state=42)
rf_cv_scores_r2 = cross_val_score(rf_model, X_scaled_df, y, cv=kf, scoring='r2', n_jobs=-1)
rf_cv_scores_mae = cross_val_score(rf_model, X_scaled_df, y, cv=kf, scoring='neg_mean_absolute_error', n_jobs=-1)

print(f"Random Forest Cross-Validation R2 Scores: {rf_cv_scores_r2}")
print(f"Random Forest Average CV R2 Score: {rf_cv_scores_r2.mean():.4f} (+/- {rf_cv_scores_r2.std():.4f})")
print(f"Random Forest Cross-Validation MAE Scores (negative for scoring): {-rf_cv_scores_mae}")
print(f"Random Forest Average CV MAE: {-rf_cv_scores_mae.mean():.4f} (+/- {rf_cv_scores_mae.std():.4f})")


# Feature Importances from Random Forest
print("\nRandom Forest Feature Importances:")
feature_importances_rf = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)
print(feature_importances_rf)

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x=feature_importances_rf.values, y=feature_importances_rf.index)
plt.title('Random Forest Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()

# --- Model 2: K-Nearest Neighbors (KNN) Regressor ---
print("\n--- Training K-Nearest Neighbors (KNN) Regressor ---")
# It's important to use scaled data for KNN
knn_model = KNeighborsRegressor(n_neighbors=5) # n_neighbors is a hyperparameter
knn_model.fit(X_train, y_train)
print("KNN model trained.")

# Predictions
y_pred_knn = knn_model.predict(X_test)

# Evaluation
mae_knn = mean_absolute_error(y_test, y_pred_knn)
mse_knn = mean_squared_error(y_test, y_pred_knn)
rmse_knn = np.sqrt(mse_knn)
r2_knn = r2_score(y_test, y_pred_knn)

print(f"KNN MAE: {mae_knn:.4f}")
print(f"KNN MSE: {mse_knn:.4f}")
print(f"KNN RMSE: {rmse_knn:.4f}")
print(f"KNN R2 Score: {r2_knn:.4f}")

# Cross-validation for KNN
print("\nPerforming 5-fold cross-validation for KNN...")
knn_cv_scores_r2 = cross_val_score(knn_model, X_scaled_df, y, cv=kf, scoring='r2', n_jobs=-1)
knn_cv_scores_mae = cross_val_score(knn_model, X_scaled_df, y, cv=kf, scoring='neg_mean_absolute_error', n_jobs=-1)

print(f"KNN Cross-Validation R2 Scores: {knn_cv_scores_r2}")
print(f"KNN Average CV R2 Score: {knn_cv_scores_r2.mean():.4f} (+/- {knn_cv_scores_r2.std():.4f})")
print(f"KNN Cross-Validation MAE Scores (negative for scoring): {-knn_cv_scores_mae}")
print(f"KNN Average CV MAE: {-knn_cv_scores_mae.mean():.4f} (+/- {knn_cv_scores_mae.std():.4f})")


# --- 5. Conclusion and Comparison ---
print("\n--- Model Comparison ---")
print(f"Random Forest R2 Score: {r2_rf:.4f} | MAE: {mae_rf:.4f}")
print(f"KNN Regressor R2 Score: {r2_knn:.4f} | MAE: {mae_knn:.4f}")

print("\nBased on these metrics, you can choose the model that performs better for your specific needs.")
print("Further steps could include hyperparameter tuning, more advanced feature engineering, and exploring other models.")

